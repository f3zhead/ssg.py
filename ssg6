#!/bin/sh -e

# Copyright (c) 2021 parabolas.xyz
#
#  This file is free software: you may copy, redistribute and/or modify it
#  under the terms of the GNU General Public License as published by the
#  Free Software Foundation, either version 3 of the License, or (at your
#  option) any later version.
#
#  This file is distributed in the hope that it will be useful, but
#  WITHOUT ANY WARRANTY; without even the implied warranty of
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
#  General Public License for more details.
#
#  You should have received a copy of the GNU General Public License
#  along with this program.  If not, see <https://www.gnu.org/licenses/>.
#
# This file incorporates work covered by the following copyright and
# permission notice:
#
#    https://rgz.ee/bin/ssg6
#    Copyright 2018-2019 Roman Zolotarev <hi@romanzolotarev.com>
#
#    Permission to use, copy, modify, and/or distribute this software for any
#    purpose with or without fee is hereby granted, provided that the above
#    copyright notice and this permission notice appear in all copies.
#
#    THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
#    WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
#    MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
#    ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
#    WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
#    ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
#    OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.

main() {
	test -n "$1" || usage
	test -n "$2" || usage
	test -n "$3" || usage
	test -n "$4" || usage
	test -d "$1" || no_dir "$1"
	test -d "$2" || no_dir "$2"

	src=$(readlink_f "$1")
	dst=$(readlink_f "$2")

	IGNORE=$(
		if ! test -f "$src/.ssgignore"; then
			printf ' ! -path "*/.*"'
			return
		fi
		while read -r x; do
			test -n "$x" || continue
			printf ' ! -path "*/%s*"' "$x"
		done <"$src/.ssgignore"
	)

	# files

	title="$3"

	h_file="$src/_header.html"
	f_file="$src/_footer.html"
	test -f "$f_file" && FOOTER=$(cat "$f_file") && export FOOTER
	test -f "$h_file" && HEADER=$(cat "$h_file") && export HEADER

	list_dirs "$src" |
		(cd "$src" && cpio -pdu "$dst")

	fs=$(
		if test -f "$dst/.files"; then
			list_affected_files "$src" "$dst/.files"
		else
			list_files "$1"
		fi
	)

	if test -n "$fs"; then
		echo "$fs" | tee "$dst/.files"

		if echo "$fs" | grep -q '\.md$'; then
			if test -x "$(which pandoc 2>/dev/null)"; then
				echo "$fs" | grep '\.md$' |
					render_md_files_pandoc "$src" "$dst" "$title"
			else
				echo "couldn't find pandoc"
				exit 3
			fi
		fi

		echo "$fs" | grep '\.html$' |
			render_html_files "$src" "$dst" "$title"

		echo "$fs" | grep -Ev '\.md$|\.html$' |
			(cd "$src" && cpio -pu "$dst")
	fi

	printf '[ssg] ' >&2
	print_status 'file' 'files' "$fs" >&2
	echo # move file list to next line

	# sitemap

	base_url="$4"
	date=$(date +%Y-%m-%d)
	urls=$(list_pages "$src")

	test -n "$urls" &&
		render_sitemap "$urls" "$base_url" "$date" >"$dst/sitemap.xml"
		render_article_list "$urls" "$base_url" "$dst" "$src"
		render_rss "$urls" "$base_url" "$dst" "$src"
		render_atom "$urls" "$base_url" "$dst" "$src"

	print_status 'url' 'urls' "$urls" >&2
	echo >&2
}

readlink_f() {
	file="$1"
	cd "$(dirname "$file")"
	file=$(basename "$file")
	while test -L "$file"; do
		file=$(readlink "$file")
		cd "$(dirname "$file")"
		file=$(basename "$file")
		echo $file
	done
	dir=$(pwd -P)
	echo "$dir/$file"
}

print_status() {
	test -z "$3" && printf 'no %s' "$2" && return

	echo "$3" | awk -v singular="$1" -v plural="$2" '
	END {
		if (NR==1) printf NR " " singular
		if (NR>1) printf NR " " plural
	}'
}

usage() {
	echo "usage: ${0##*/} src dst title base_url" >&2
	exit 1
}

no_dir() {
	echo "${0##*/}: $1: No such directory" >&2
	exit 2
}

list_dirs() {
	cd "$1" && eval "find . -type d ! -name '.' ! -path '*/_*' $IGNORE"
}

list_files() {
	cd "$1" && eval "find . -type f ! -name '.' ! -path '*/_*' $IGNORE"
}

list_dependant_files() {
	e="\\( -name '*.html' -o -name '*.md' -o -name '*.css' -o -name '*.js' \\)"
	cd "$1" && eval "find . -type f ! -name '.' ! -path '*/_*' $IGNORE $e"
}

list_newer_files() {
	cd "$1" && eval "find . -type f ! -name '.' $IGNORE -newer $2"
}

has_partials() {
	grep -qE '^./_.*\.html$|^./_.*\.js$|^./_.*\.css$'
}

list_affected_files() {
	fs=$(list_newer_files "$1" "$2")

	if echo "$fs" | has_partials; then
		list_dependant_files "$1"
	else
		echo "$fs"
	fi
}

render_html_files() {
	while read -r f; do
		render_html_file "$3" <"$1/$f" >"$2/$f"
	done
}

render_toc() {
	# remove code blocks
	input=$(echo "$1" | sed '/```/,/```/d')
	toc=$(echo "$input" | grep -E "^#{1,5} " | sed -E 's/(#+) (.+)/\1:\2:\2/g' | awk -F ":" '{ gsub(/#/,"  ",$1); gsub(/[ ]/,"-",$3); print $1 "- [" $2 "](#" tolower($3) ")" }' | pandoc | sed ' 1 i<nav class="toc">' | sed '$a</nav>')
	echo "$toc"
}

render_md_files_pandoc() {
	while read -r f; do
		if ! echo "$f" | grep -q "index" 2>/dev/null ; then
			DATE=$(grep -m 1 "Date:" "$1/$f" | awk -F ": " '{ printf $2 }')
#			page_date=$(date -u -d "$page_date" +"%B %d, %Y")
			DATE=$(date -u -d "$page_date" +"%Y-%m-%d")
			TITLE=$(grep -m 1 "Title: " "$1/$f" | awk -F ": " '{ printf $2 }')
			header=$(printf "%s\n%s\n" "# ${TITLE}" "<p class=\"date\">${DATE}</p>")
			clean_file="$(cat "$1/$f" | tail -n +2 | grep -A100000 '\-\-\-' | tail -n +2)" # remove --- header ---
			complete_file=$(echo "$header" "$clean_file")
			toc=$(render_toc "$complete_file")
			printf "%s\n\n%s" "$toc" "$complete_file" | \
			#echo "$complete_file" | \
			pandoc | render_html_file "$3" \
					>"$2/${f%\.md}.html"
		else
			# dont need the date and title header for index.html
			pandoc "$1/$f" | render_html_file "$3" \
				> "$2/${f%\.md}.html"
		fi
	done
}

render_html_file() {
	# h/t Devin Teske
	awk -v title="$1" '
	{ body = body "\n" $0 }
	END {
		body = substr(body, 2)
		if (body ~ /<\/?[Hh][Tt][Mm][Ll]/) {
			print body
			exit
		}
		if (match(body, /<[[:space:]]*[Hh]1(>|[[:space:]][^>]*>)/)) {
			t = substr(body, RSTART + RLENGTH)
			sub("<[[:space:]]*/[[:space:]]*[Hh]1.*", "", t)
			gsub(/^[[:space:]]*|[[:space:]]$/, "", t)
			if (t) title = t " &mdash; " title
		}
		n = split(ENVIRON["HEADER"], header, /\n/)
		for (i = 1; i <= n; i++) {
			if (match(tolower(header[i]), "<title></title>")) {
				head = substr(header[i], 1, RSTART - 1)
				tail = substr(header[i], RSTART + RLENGTH)
				print head "<title>" title "</title>" tail
			} else print header[i]
		}
		print body
		print ENVIRON["FOOTER"]
	}'
}

list_pages() {
	e="\\( -name '*.html' -o -name '*.md' \\)"
	cd "$1" && eval "find . -type f ! -path '*/.*' ! -path '*/_*' $IGNORE $e" |
		sed 's#^./##;s#.md$#.html#;s#/index.html$#/#'
}

render_article_list() {
	urls="$1"
	base_url="$2"
	items=""
	for i in $1; do
		if ! echo $i | grep -Eq "index|contact"; then
			page_date=$(grep -m 1 "Date:" "$4/${i%\.html}.md" | awk -F ": " '{ printf $2 }')
			page_date=$(date -u -d "$page_date" +"%s")
			url="$page_date:$i"
			urls_sorted="$(printf "$urls_sorted\n$url" | sort -r)"
		fi
	done

	for i in $urls_sorted; do
		if ! echo $i | grep -Eq "index|contact"; then
			url=$(echo $i | cut -d: -f2-)
			echo $url
			page_title=$(grep -m 1 "Title: " "$4/${url%\.html}.md" | awk -F ": " '{ printf $2 }')
			page_date=$(grep -m 1 "Date: " "$4/${url%\.html}.md" | awk -F ": " '{ printf $2 }')
#			page_date=$(date -u -d "$page_date" +"%B %d, %Y")
			page_date=$(date -u -d "$page_date" +"%Y-%m-%d")
			item="<li><a href="\"${url}\"">${page_title}</a><p class=\"date\">${page_date}</p></li>"
			items=$items$item
		fi
	done

	# don't touch article list if same otherwise replace
	if ! grep -q "<ul class=\"articles\">${items}</ul>" "$3/index.html"; then
		sed -i -e "s|.*<ul class=\"articles\">.*|<ul class=\"articles\">${items}</ul></article>|g" "$3/index.html"
	fi

	# if no article list generate one
	if ! grep -q "<ul class=\"articles\">" "$3/index.html"; then
		sed -i -e "s|</article>|<ul class=\"articles\">${items}</ul></article>|g" "$3/index.html"
	fi

	#	sed -i -e "s|</article>|<ul class="articles">${items}</ul></article>|g" "$3/index.html" || echo "Run \'make clean\'"
}

render_sitemap() {
	urls="$1"
	base_url="$2"
	date="$3"

	echo '<?xml version="1.0" encoding="UTF-8"?>'
	echo '<urlset'
	echo 'xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"'
	echo 'xsi:schemaLocation="http://www.sitemaps.org/schemas/sitemap/0.9'
	echo 'http://www.sitemaps.org/schemas/sitemap/0.9/sitemap.xsd"'
	echo 'xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">'
	echo "$urls" |
		sed -E 's#^(.*)$#<url><loc>'"$base_url"'/\1</loc><lastmod>'"$date"'</lastmod><priority>1.0</priority></url>#'
	echo '</urlset>'
}

render_rss() { # im sorry this is spaghetti code
	urls="$1"
	base_url="$2"
	src_path="$4"
	rss_file="${dst}/rss.xml"
	BLOG_FEED_MAX='50'
	BLOG_DESCRIPTION="blog"
	printf '<?xml version="1.0" encoding="UTF-8"?>\n<rss version="2.0">\n<channel>\n<title>%s</title>\n<link>%s</link>\n<description>%s</description>\n' \
		"${title}" "${base_url}" "${BLOG_DESCRIPTION}" > $rss_file

	for i in $(echo $urls_sorted | head -n ${BLOG_FEED_MAX}) ; do
		if ! echo $i | grep -Eq "index|contact"; then
			url=$(echo $i | cut -d: -f2-)
			date=$(echo $i | cut -d: -f1)
			FILE="${dst}/${url}"
			echo $FILE
			printf '<item>\n<title>%s</title>\n<link>%s</link>\n<guid>%s</guid>\n<pubDate>%s</pubDate>\n<description><![CDATA[%s]]></description>\n</item>\n' \
			"$(awk -vRS="</title>" '/<title>/{gsub(/.*<title>|\n+/,"");print;exit}' $FILE | sed -s "s/&mdash;/-/g")" \
			"${base_url}/`basename $FILE`" \
			"${base_url}/`basename $FILE`" \
			"$(date -ud @${date} +"%a, %d %b %Y %R:%S %z")" \
			"$(tr "\n" "|" < $FILE | grep -o '<article>.*</article>' | sed 's/\(<article>\|<\/article>\)//g;s/|/\n/g')" >> $rss_file
		fi
	done
	printf '</channel>\n</rss>\n' >> $rss_file
}

render_atom() {
	urls="$1"
	base_url="$2"
	src_path="$4"
	atom_file="${dst}/atom.xml"

	printf '<?xml version="1.0" encoding="UTF-8"?>\n<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">\n<title type="text">%s</title>\n<subtitle type="text">%s</subtitle>\n<updated>%s</updated>\n<link rel="alternate" type="text/html" href="%s"/>\n<id>%s</id>\n<link rel="self" type="application/atom+xml" href="%s"/>\n' \
		"${title}" "${BLOG_DESCRIPTION}" "$(date +%Y-%m-%dT%H:%M:%SZ)" "${base_url}" "${base_url}/atom.xml" "${base_url}/atom.xml" > $atom_file

	for i in $(echo $urls_sorted | head -n ${BLOG_FEED_MAX}) ; do
		if ! echo $i | grep -Eq "index|contact"; then
			url=$(echo $i | cut -d: -f2-)
			date=$(echo $i | cut -d: -f1)
			FILE="${dst}/${url}"
			SOURCE="${src}/$(echo $url | sed -s "s/html/md/g")"
			AUTHOR="parabolas.xyz"
			printf '<entry>\n<title type="text">%s</title>\n<link rel="alternate" type="text/html" href="%s"/>\n<id>%s</id>\n<published>%s</published>\n<updated>%s</updated>\n<author><name>%s</name></author>\n<summary type="html"><![CDATA[%s]]></summary>\n</entry>\n' \
				"$(awk -vRS="</title>" '/<title>/{gsub(/.*<title>|\n+/,"");print;exit}' $FILE | sed -s "s/&mdash;/-/g")" \
				"${base_url}/`basename $FILE`" \
				"${base_url}/`basename $FILE`" \
				"$(date -d @${date} +%Y-%m-%dT%H:%M:%SZ )" \
				"`git log -n 1 --date="format:%Y-%m-%dT%H:%M:%SZ" --pretty=format:'%ad' -- "$SOURCE"`" \
				"$AUTHOR" \
				"$(tr "\n" "|" < $FILE | grep -o '<article>.*</article>' | sed 's/\(<article>\|<\/article>\)//g;s/|/\n/g')" >> $atom_file
		fi
	done
	printf '</feed>\n' >> $atom_file
}

main "$@"
